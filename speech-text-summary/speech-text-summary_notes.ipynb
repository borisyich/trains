{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34b4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "import ollama\n",
    "import os\n",
    "import pydub\n",
    "import telegram\n",
    "import torch\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from telegram.ext import Application, CommandHandler, MessageHandler, filters\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de9cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2da387",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "TELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "if not TELEGRAM_BOT_TOKEN:\n",
    "    raise ValueError(\"TELEGRAM_BOT_TOKEN not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9c312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Инициализация моделей\n",
    "whisper = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device=0)\n",
    "punctuation_model = pipeline(\"token-classification\", model=\"oliverguhr/fullstop-punctuation-multilang-large\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98be4b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Гудтаймс - Натальные карты.wav'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path =  \"Гудтаймс - Натальные карты.mp3\"\n",
    "wav_path = file_path.replace(\".mp3\", \".wav\")\n",
    "audio = pydub.AudioSegment.from_mp3(file_path)\n",
    "audio.export(wav_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edafd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    }
   ],
   "source": [
    "result = whisper(wav_path, return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad35651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Киса Свои планы, тебя в этих планах нет Тебя в этих планах нет Данные на тайных карт И астрологов прогнозы Старо под угрозу наш союз У вселенной есть свой план Если верить гороскопам Мы совместимы, я варю Я всё понимаю, киса в это верится с трудом У вселенной свои планы, ты поймёшь это потом С этим ничего не сделать, ничего не изменить Всё предрешено от самой встречи по этот миг Дай мне на проникать И астрологов прогнозы Старом под угрозу нас дают У Вселенной есть свой план Если верить гороскопам Мы совместимы, я боюсь Данные на тайных карт И астрологов прогнозы Старо пахнут грозы в наш союз У вселенной есть свой план Если верить гороскопам Мы не совместимы, я боюсь Я боюсь Субтитры создавал DimaTorzok'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169c81c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='5440656767170147977.wav'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"5440656767170147977.ogg\"\n",
    "wav_path = file_path.replace(\".ogg\", \".wav\")\n",
    "audio = pydub.AudioSegment.from_ogg(file_path)\n",
    "audio.export(wav_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab274541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    }
   ],
   "source": [
    "result = whisper(wav_path, return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67eaad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Привет! Прослушай это сообщение и установи, пожалуйста, пунктуацию здесь.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9880457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION_MAP = {\n",
    "    \"COMMA\": \", \",\n",
    "    \"PERIOD\": \". \",\n",
    "    \"QUESTION\": \"? \",\n",
    "    \"EXCLAMATION\": \"! \",\n",
    "    \"NONE\": \" \"\n",
    "}\n",
    "\n",
    "def split_text(text, max_length=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1\n",
    "        if current_length > max_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def restore_punctuation(text, punctuation_model):\n",
    "    if not text or not text.strip():\n",
    "        logging.warning(\"Empty text passed to restore_punctuation\")\n",
    "        return text\n",
    "    try:\n",
    "        result = \"\"\n",
    "        for chunk in split_text(text):\n",
    "            # Tokenize and predict punctuation for each chunk\n",
    "            predictions = punctuation_model(chunk, aggregation_strategy=\"simple\")\n",
    "            if not predictions:\n",
    "                logging.warning(f\"Punctuation model returned empty result for text: {chunk}\")\n",
    "                result += chunk + \" \"\n",
    "                continue\n",
    "            chunk_result = \"\"\n",
    "            capitalize_next = True  # Capitalize first word of the chunk\n",
    "            for pred in predictions:\n",
    "                if not isinstance(pred, dict) or 'word' not in pred or 'entity' not in pred:\n",
    "                    logging.error(f\"Invalid prediction format: {pred}\")\n",
    "                    continue\n",
    "                word = pred[\"word\"]\n",
    "                punctuation = pred[\"entity\"]\n",
    "                # Capitalize word if needed\n",
    "                if capitalize_next:\n",
    "                    word = word.capitalize()\n",
    "                    capitalize_next = False\n",
    "                chunk_result += word\n",
    "                # Add corresponding punctuation\n",
    "                chunk_result += PUNCTUATION_MAP.get(punctuation, \" \")\n",
    "                # Capitalize next word after sentence-ending punctuation\n",
    "                if punctuation in [\"PERIOD\", \"QUESTION\", \"EXCLAMATION\"]:\n",
    "                    capitalize_next = True\n",
    "            result += chunk_result + \" \"\n",
    "        return result.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in punctuation restoration: {e}, text: {text}\")\n",
    "        return text  # Return original text on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5d086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Папка для сохранения файлов\n",
    "OUTPUT_DIR = \"transcriptions\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "async def start(update, context):\n",
    "    await update.message.reply_text(\"Отправьте голосовое сообщение (длиннее 10 минут), и я преобразую его в текст, восстановлю пунктуацию и создам саммари.\")\n",
    "\n",
    "async def handle_voice(update, context):\n",
    "    voice = update.message.voice\n",
    "\n",
    "    # Download voice message\n",
    "    file = await context.bot.get_file(voice.file_id)\n",
    "    file_path = os.path.join(OUTPUT_DIR, f\"voice_{voice.file_id}.ogg\")\n",
    "    await file.download_to_drive(file_path)\n",
    "\n",
    "    # Convert OGG to WAV for Whisper model compatibility\n",
    "    wav_path = file_path.replace(\".ogg\", \".wav\")\n",
    "    audio = pydub.AudioSegment.from_ogg(file_path)\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "    # Transcribe audio to text\n",
    "    logging.info(\"Starting transcription...\")\n",
    "    result = whisper(wav_path, return_timestamps=True)\n",
    "    text = result[\"text\"]\n",
    "    chunks = result[\"chunks\"]  # Segments with timestamps\n",
    "\n",
    "    # Restore punctuation in transcribed text\n",
    "    logging.info(\"Restoring punctuation...\")\n",
    "    punctuated_text = restore_punctuation(text, punctuation_model)\n",
    "\n",
    "    # Format transcription with timestamps\n",
    "    timestamped_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        start_time = chunk[\"timestamp\"][0]\n",
    "        end_time = chunk[\"timestamp\"][1]\n",
    "        chunk_text = chunk[\"text\"]\n",
    "        # Restore punctuation for each chunk\n",
    "        punctuated_chunk = restore_punctuation(chunk_text, punctuation_model)\n",
    "        timestamped_text += f\"[{start_time:.2f} - {end_time:.2f}] {punctuated_chunk}\\n\"\n",
    "\n",
    "    # Summarize text using Gemma3n via Ollama\n",
    "    logging.info(\"Summarizing text...\")\n",
    "    summary_prompt = f\"Write a summary of the text in 3-5 sentences in Russian:\\n{punctuated_text}\"\n",
    "    try:\n",
    "        response = ollama.generate(model=\"gemma3n\", prompt=summary_prompt, options={\"num_predict\": 200})\n",
    "        summary = response[\"response\"]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during summarization: {e}\")\n",
    "        summary = \"Error generating summary.\"\n",
    "\n",
    "    # Save transcription and summary to Markdown file\n",
    "    timestamp = datetime.now()\n",
    "    timestamp_for_name = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    timestamp_for_summary = timestamp.strftime('%B %d, %Y')\n",
    "    md_file = os.path.join(OUTPUT_DIR, f\"transcription_{timestamp_for_name}.md\")\n",
    "    with open(md_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# Transcription of Voice Message from {timestamp_for_summary}\\n\\n\")\n",
    "        f.write(\"## Full Text\\n\")\n",
    "        f.write(timestamped_text)\n",
    "        f.write(\"\\n## Summary\\n\")\n",
    "        f.write(summary)\n",
    "\n",
    "    # Send results to user\n",
    "    await update.message.reply_text(f\"Transcription completed. Full text:\\n{punctuated_text}\\n\\nSummary:\\n{summary}\")\n",
    "    with open(md_file, \"rb\") as f:\n",
    "        await context.bot.send_document(chat_id=update.message.chat_id, document=f)\n",
    "\n",
    "    # Clean up temporary files\n",
    "    os.remove(file_path)\n",
    "    os.remove(wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346a4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to set up and run the Telegram bot.\n",
    "    \"\"\"\n",
    "    # Initialize Telegram bot with token\n",
    "    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()\n",
    "    # Register command and message handlers\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    application.add_handler(MessageHandler(filters.VOICE, handle_voice))\n",
    "    # Start polling for updates\n",
    "    await application.run_polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381d870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
